{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Identifying Strongly-Lensed Graviational-Wave Signals using `hanabi`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "`hanabi`, which stands for **H**ierarchical bayesian **ANA**lysis on lensed GW signals using **BI**lby, implements a hierarchical Bayesian approach that accounts for both selection effects and population information to identify strongly-lensed GW signals using the bilby codebase. Here we will briefly explain the statistical framework. For more details on the methodology, see *A Bayesian Statistical Framework for Identifying Strongly-Lensed Gravitational-Wave Signals* by Lo et al. (to be published/posted to arXiv).\n",
    "\n",
    "The basic idea is to compute a statistic, commonly referred as a *Bayes factor*, that allows us to choose one out of the two competing hypotheses that claim to describe the data that we observed. In our case, where we try to discern strongly-lensed gravitational-wave signals, the two hypotheses would be the lensed hypothesis $\\mathcal{H}_{\\rm L}$ and the not-lensed hypothesis $\\mathcal{H}_{\\rm NL}$. In particular, if there are two GW events under consideration, then the two hypotheses mean that\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{H}_{\\rm L} & : & \\textrm{The two GW signals are lensed images coming from the same source} \\\\\n",
    "\\mathcal{H}_{\\rm NL} & : & \\textrm{The two GW signals came from two separate sources}\n",
    "\\end{aligned}\n",
    ",\n",
    "$$\n",
    "where we see that whether or not two signals are lensed would depend on the particular source population (model) one assumes.\n",
    "\n",
    "The Bayes factor $\\mathcal{B}$ is given by the ratio of the probability (density) of observing the data set of interest $\\mathbf{D}$ under the two hypotheses, namely\n",
    "\n",
    "$$\n",
    "\\mathcal{B} = \\frac{p(\\mathbf{D}|\\mathcal{H}_{\\rm L})}{p(\\mathbf{D}|\\mathcal{H}_{\\rm NL})}.\n",
    "$$\n",
    "\n",
    "Note that the probability densities $p(\\mathbf{D}|\\mathcal{H}_{\\rm L}), p(\\mathbf{D}|\\mathcal{H}_{\\rm NL})$, which are also known as the **evidence**, should be normalized (proper). This is where selection effect comes in -- the selection functions serve as the normalization constants for each of the hypothesis. In order to compute the probability densities of observing the data $\\mathbf{D}$, one will need to have a population model for the source, and a lensing model. Optionally, instead of fixing one particular source population model and lensing model, it is straightforward to consider a family of source population models and lensing models parametrized by some 'hyper-parameters' and marginalize over the hyper-parameters.\n",
    "\n",
    "\n",
    "In the following notebook, we demonstrate how to use the `hanabi` package to compute the (proper) Bayes factor. Optionally, one can multiply the Bayes factor with an additional \"timing Bayes factor\" to leverage our prior knowledge about the time delay of lensed signals, but we will not be including the calculation in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Loading libraries\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "import bilby\n",
    "from hanabi.hierarchical.source_population_model import *\n",
    "from hanabi.hierarchical.merger_rate_density import *\n",
    "from hanabi.hierarchical.selection_function import *\n",
    "from hanabi.hierarchical.p_z import *\n",
    "from hanabi.hierarchical.result import *\n",
    "from hanabi.lensing.optical_depth import *\n",
    "from hanabi.lensing.absolute_magnification import *\n",
    "from hanabi.hierarchical.marginalized_likelihood import *\n",
    "from hanabi.hierarchical.gibbs_sampling import CustomCollapsedBlockedGibbsSampler\n",
    "from hanabi.hierarchical.reweight_with_population_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "defaults_kwargs = {\n",
    "    'bins': 50,\n",
    "    'smooth': 0.9,\n",
    "    'label_kwargs': {'fontsize': 16},\n",
    "    'title_kwargs': {'fontsize': 16},\n",
    "    'color': '#0072C1',\n",
    "    'truth_color': 'tab:orange',\n",
    "    'quantiles': [0.16, 0.84],\n",
    "    'levels': (1 - np.exp(-0.5), 1 - np.exp(-2), 1 - np.exp(-9 / 2.)),\n",
    "    'plot_density': False,\n",
    "    'plot_datapoints': True,\n",
    "    'fill_contours': True,\n",
    "    'max_n_ticks': 3\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Loading PE results\n",
    "\n",
    "First, we read in the joint-PE result, which was generated using `hanabi.inference`, and the single-PE results, which were generated using `bilby_pipe`. After a PE run has been completed, both will generate a JSON file containing the inference result such as the posterior samples and the evidence.\n",
    "\n",
    "\n",
    "The `hanabi.hierarchical` module **requires that the component-mass parametrization is used**. If a different mass parametrization (oftentimes chirp_mass-mass_ratio) was used during the sampling instead, you can run\n",
    "```bash\n",
    "$ hanabi_postprocess_result --flat-in-component-masses PATH_TO_RESULT_JSON_FILE\n",
    "```\n",
    "to convert a `bilby` result using the chirp_mass-mass_ratio parametrization to the component-mass parametrization, taking the Jacobian into account. Note that the prior for the component masses does not have to be flat/uniform.\n",
    "\n",
    "The module also **requires the apparent luminosity distance of each signal/image is being sampled**. The apparent luminosity distance of the `i`-th image (`i=1,2,...`) should be called `luminosity_distance^(i)` as its name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Loading PE results\n",
    "rundir = \"RUNDIR\"\n",
    "event_1 = \"trigger_1\"\n",
    "event_2 = \"trigger_2\"\n",
    "\n",
    "single_event_PE_result_path_template = \"{rundir}/highmass-lensed-{event}_0_reweighted_result.json\"\n",
    "joint_PE_result_path_template = \"{rundir}/joint_{event_1}_{event_2}_0_reweighted_result.json\"\n",
    "\n",
    "result_1 = bilby.result.read_in_result(single_event_PE_result_path_template.format(rundir=rundir, event=event_1))\n",
    "result_2 = bilby.result.read_in_result(single_event_PE_result_path_template.format(rundir=rundir, event=event_2))\n",
    "joint_result = bilby.result.read_in_result(joint_PE_result_path_template.format(rundir=rundir, event_1=event_1, event_2=event_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Computing the coherence ratio\n",
    "\n",
    "One can compute the so-called 'coherence ratio' as an intermediate statistic, which is the *unnormalized/improper* Bayes factor under a particular set of sampling priors **without** selection effects and population information. It cannot/should not be interpreted as a ratio of probability like a proper Bayes factor. For example, a coherence ratio of 10 does not mean that the lensed hypothesis is ten times more favored by the observed data than the not-lensed hypothesis. However, **a negative log coherence ratio would indicate that the observed data set cannot be explained by a common set of intrinsic parameters** (and some of the extrinsic parameters such as the sky location) as in the case if the events are truly lensed. That is, the events are 'incoherent' and hence the name of the statistic.\n",
    "\n",
    "Here we calculate the log coherence ratio from the PE results and print out its value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "lcr = compute_log_coherence_ratio(joint_result, result_1, result_2)\n",
    "print(lcr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Setting up the population and lensing model\n",
    "In order to compute the evidence under the lensed and not-lensed hypothesis, one will need to provide a *source population model*, which **describes the fraction of sources satisfying some properties** (hence normalized), and a *lensing model*, which **describes the fraction of sources that is lensed with images satisfying some properties** (again normalized).\n",
    "\n",
    "<font color='red'><b>[Change the following before public release to reflect what was being used]</b></font>\n",
    "\n",
    "Here we use the following population model for masses\n",
    "\n",
    "$$\n",
    "p(m_1^{\\rm src}, m_2^{\\rm src}|\\alpha, \\beta, m_{\\rm min}, m_{\\rm max}) = \n",
    "\\begin{cases}\n",
    "\\frac{1 - \\alpha}{m_{\\rm max}^{1-\\alpha} - m_{\\rm min}^{1-\\alpha}} m_1^{-\\alpha} \\frac{1 + \\beta}{(m_{1}^{\\rm src})^{1+\\beta} - m_{\\rm min}^{1+\\beta}} m_2^{\\beta} & \\textrm{if } m_{\\rm min} \\leq m_2^{\\rm src} \\leq m_1^{\\rm src} \\leq m_{\\rm max} \\\\\n",
    "0 & \\textrm{otherwise}\n",
    "\\end{cases},\n",
    "$$\n",
    "\n",
    "with $\\alpha=1.8, \\beta=0, m_{\\rm min}=5 M_{\\odot}, m_{\\rm max}=60 M_{\\odot} $.\n",
    "\n",
    "For spin, we are using the isotropic spin distribution, which is the same as the sampling distribution for the spins.\n",
    "\n",
    "For the merger rate density $\\mathcal{R}(z)$, which in turns controls how the source redshift $z$ is distributed. Here we use a merger rate density model tracking the star-formation rate of *Belczynski et al. 2017* for population-I and population-II stars, namely\n",
    "\n",
    "$$\n",
    "\\mathcal{R}(z) = \\frac{6.6\\times10^{3}\\exp(1.6z)}{30+\\exp(2.1z)}.\n",
    "$$\n",
    "\n",
    "Note that here we use the pre-defined models in `hanabi.hierarchical.source_population_model` but users are free to use any compatible population model (see [here](https://git.ligo.org/ka-lok.lo/hanabi/-/blob/master/hanabi/hierarchical/source_population_model.py) for more information).\n",
    "\n",
    "\n",
    "As for the lensing model, we assume that the absolute magnification of an image $\\mu$ follows the following probability distribution\n",
    "\n",
    "$$\n",
    "p(\\mu) \\propto \n",
    "\\begin{cases}\n",
    "\\mu^{-3} & \\textrm{if }\\mu \\geq 2 \\\\\n",
    "0 & \\textrm{otherwise}\n",
    "\\end{cases},\n",
    "$$\n",
    "\n",
    "pre-defined in `hanabi.lensing.absolute_magnification` and adopt an optical depth $\\tau(z)$ (the probability of strong lensing for a source at redshift $z$) from *Hannuksela et al. 2019*, namely\n",
    "\n",
    "$$\n",
    "\\tau(z) = F \\left( \\frac{d_{\\rm C}(z)}{d_{\\rm H}} \\right)^{3},\n",
    "$$\n",
    "\n",
    "where $F$ is an empirical constant taken to be $0.0017$ here, $d_{\\rm H}$ is the Hubble distance and $d_{\\rm C}(z)$ is the comoving distance. This model is pre-defined in `hanabi.lensing.optical_depth`.\n",
    "\n",
    "Again, users are free to use any compatible model. For the absolute magnification, users can provide any `bilby` prior. For the optical depth, users can provide any class that implements a function `evaluate(z)` that returns the optical depth at $z$ (see [here](https://git.ligo.org/ka-lok.lo/hanabi/-/blob/master/hanabi/lensing/optical_depth.py) for more information)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Setting up the population models\n",
    "mass_src_pop_model = PowerLawComponentMass(alpha=1.8, beta=0, mmin=5, mmax=60)\n",
    "spin_src_pop_model = None # Spin information is not being used\n",
    "merger_rate_density = BelczynskiEtAl2017PopIPopIIStarsBBHMergerRateDensity()\n",
    "\n",
    "# Setting up the strong lensing models\n",
    "optical_depth = HannukselaEtAl2019OpticalDepth()\n",
    "pz_lensed = LensedSourceRedshiftProbDist(merger_rate_density=merger_rate_density, optical_depth=optical_depth)\n",
    "pz_notlensed = NotLensedSourceRedshiftProbDist(merger_rate_density=merger_rate_density, optical_depth=optical_depth)\n",
    "abs_magnifications = [SISPowerLawAbsoluteMagnification(), SISPowerLawAbsoluteMagnification()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Computing the (unnormalized) evidence under the not-lensed hypothesis\n",
    "\n",
    "We now compute the (unnormalized) evidence under the not-lensed hypothesis from the single-event PE results. Note that the aforementioned degeneracy does not exist under the not-lensed hypothesis. Therefore, the calculation of the evidence will not involve any extra marginalization over redshift but a simple reweighting with the given source population model, as we can compute $z = z(d_{\\rm L})$ as a function of the luminosity distance $d_{\\rm L}$. This reweighting is implemented in `hanabi.hierarchical.reweight_with_population_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Reweighting single-event results with population models\n",
    "reweighting_result_1 = ReweightWithPopulationModel(result_1, mass_src_pop_model, spin_src_pop_model, pz_notlensed)\n",
    "reweighting_result_2 = ReweightWithPopulationModel(result_2, mass_src_pop_model, spin_src_pop_model, pz_notlensed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Computing the (unnormalized) evidence under the lensed hypothesis\n",
    "\n",
    "We first compute the (unnormalized) evidence under the lensed hypothesis from a joint-PE result.\n",
    "\n",
    "Note that during the sampling/inference, detector-frame quantities were inferred, such as the *redshiftted* masses and the *apparent* luminosity distance (which is different from the true source luminosity distance due to magnification by lensing). In general we will not be able to 'de-magnified' and infer the true source luminosity distance, and hence the true source redshift and masses due to this magnification effect.\n",
    "\n",
    "Even though we might not be able to pinpoint the exact redshift of the source, notice that given a source population model and a source redshift, one can easily work out the appropriate prior for the detector-frame quantities which are being sampled. Effectively, we **treat the source redshift $z$ as a hyper-parameter for the prior**. This is akin to the normal population analysis for GWs.\n",
    "\n",
    "Therefore our hierarchical analysis consists of only one parameter, the true source redshift $z$. The 'hyper-prior' $p_{z}(z)$ under the lensed hypothesis can be calculated from the given $\\mathcal{R}(z)$ and $\\tau(z)$, whereas the likelihood $\\mathcal{L}(z)$ can be constructed from the joint-PE result. In particular, we can think of the evidence from PE as a function of the hyper-parameter $z$ as we changes the prior for the detector-frame quantities with $z$. Since we used a Monte-Carlo algorithm (nested sampling to be exact) for PE, this likelihood $\\mathcal{L}(z)$ is aptly named Monte-Carlo marginalized likelihood.\n",
    "\n",
    "In the following, we use `bilby` with a nested sampler called `dynesty` to do this 'marginalization' over the source redshift to get the unnormalized evidence under the lensed hypothesis, which is the prime product of a nested sampling algorithm. We will correct for the normalization later on. As a 'by-product' of the nested sampling, we also get a set of posterior samples of the redshift $z \\sim p(z|\\mathbf{D})$. Later we will show how one can use Gibbs sampling to get back the desired unbiased source-frame parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "ncores = 8\n",
    "label = \"marginalize_over_redshift_{}_{}\".format(event_1, event_2)\n",
    "# Forcing multiprocessing to use the spawn method to avoid signature clash\n",
    "import multiprocessing\n",
    "multiprocessing.set_start_method(\"spawn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Setting up likelihood\n",
    "likelihood = MonteCarloMarginalizedLikelihood(joint_result, mass_src_pop_model, spin_src_pop_model, abs_magnifications)\n",
    "# Marginalizing over redshift for the joint result\n",
    "redshift_result = bilby.run_sampler(likelihood=likelihood, priors={'redshift': pz_lensed}, sampler='dynesty', nlive=1000, nact=30, npool=ncores, sample='unif', outdir=\"marginalization\", label=label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Now, we can repeat the log coherence ratio calculation, but this time with priors accounting for the population information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "log_coherence_ratio = compute_log_coherence_ratio(redshift_result, reweighting_result_1, reweighting_result_2)\n",
    "print(log_coherence_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Computing the (proper) Bayes factor\n",
    "\n",
    "With the unnormalized Bayes factor calculated above, we will need to compute the appropriate normalization constants (which are called selection functions) under each of the hypotheses. Under the not-lensed hypothesis, the two GW signals under consideration came from two separate sources. The selection function under the not-lensed hypothesis for a single source is referred as $\\alpha$. Since there are two sources, the proper normalization constant would be $1/\\alpha^2$. Similarly for the lensed hypothesis, the selection function is termed $\\beta$ and hence the proper normalization constant would be $1/\\beta$.\n",
    "\n",
    "In order to compute the properly normalized Bayes factor, one will need to multiple the value of the coherence ratio by the normalization constant $\\alpha^2/\\beta$.\n",
    "\n",
    "The value $\\ln \\beta/\\alpha^2$ provides us a scale for the log coherence ratio. This is because if the (population-weighted) log coherence ratio is smaller than this value, then the actual proper log Bayes factor would be negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Here we load a HDF5 file with selection functions $\\alpha, \\beta$ pre-computed using the package `pdetclassifier`. We provide an interface with `pdetclassifier` called `compute_selection_functions.py` under the folder `hanabi/hierarchical/pdetclassifier`.\n",
    "\n",
    "We can now calculate the proper (normalized) Bayes factor for the lensed versus the not-lensed hypothesis, which is the coherence ratio corrected for normalization as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "log_Bayes_factor = compute_log_Bayes_factor(\n",
    "                    \"/home/ka-lok.lo/projects/stronglensing/hanabi/hanabi/hierarchical/pdetclassifier/o3a/o3a_first3months_bbhpop_powerlaw_mmax_60_isotropic_spin_avg.h5\",\n",
    "                    redshift_result,\n",
    "                    reweighting_result_1,\n",
    "                    reweighting_result_2)\n",
    "\n",
    "print(log_Bayes_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Inferring unbiased source parameters using Gibbs sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Ultimately, we are interested in the true/unbiased source-frame parameters such as the true component masses and not the redshiftted masses. However, we only have a set of posterior samples for the source redshift $z \\sim p(z|\\mathbf{D})$, and a set of detector-frame parameters from the PE $\\theta \\sim p(\\theta|\\mathbf{D}, z)$. With the help of Gibbs sampling, we can draw the desired joint posterior distribution\n",
    "\n",
    "$$\n",
    "z, \\theta \\sim p(z, \\theta|\\mathbf{D}) \\propto p(\\theta|\\mathbf{D},z)p(z|\\mathbf{D})\n",
    "$$\n",
    "\n",
    "From the joint posterior samples, we can easily compute the unbiased source-frame parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Here we call the custom Gibbs sampler implemented in `hanabi.hierarchical.gibbs_sampling` and draw 10000 joint posterior samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Gibbs sampling\n",
    "sampler = CustomCollapsedBlockedGibbsSampler(redshift_result, likelihood, pool=ncores)\n",
    "joint_samples = sampler.sample(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "compute_source_parameters(joint_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import corner\n",
    "plt.rcParams.update({'text.usetex': True})\n",
    "plt.figure(dpi=150)\n",
    "fig = corner.corner(joint_samples[[\"mass_1_source\", \"mass_2_source\", \"total_mass_source\", \"magnification^(1)\", \"magnification^(2)\", \"relative_magnification^(2)\", \"redshift\"]], labels=[r'$m_1^{\\rm src}\\;[M_{\\odot}]$', r'$m_2^{\\rm src}\\;[M_{\\odot}]$', r'$M_{\\rm tol}^{\\rm src}\\;[M_{\\odot}]$', r'$\\mu^{(1)}$', r'$\\mu^{(2)}$', r'$\\mu^{(\\rm rel)}$', r'$z$'], **defaults_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "save_hierarchical_analysis_result(\n",
    "    label=\"{}_{}_hierarchical_analysis\".format(event_1, event_2),\n",
    "    log_Bayes_factor=log_Bayes_factor,\n",
    "    log_coherence_ratio=log_coherence_ratio,\n",
    "    joint_samples=joint_samples\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hanabi-dev",
   "language": "python",
   "name": "hanabi-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
